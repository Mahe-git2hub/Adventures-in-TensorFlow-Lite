{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileDet_Conversion_TFLite.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMA7bmBvaTlUJ42erQ4v6l2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/MobileDet_Conversion_TFLite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rz1wbDv58t2",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we will see how to convert a pre-trained [MobileDet](https://arxiv.org/abs/2004.14525) model to TensorFlow Lite (using the latest converter). As the MobileDet model we are going to be using was trained in TensorFlow 1 we first need to generate a TensorFlow Lite compatible graph file following the instructions from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md). \n",
        "\n",
        "**WIP**: Include direct TFLite model download links so that users can directly try them out. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2eGYCFBpn3I",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Initial setup\n",
        "# Because MobileDet is based out of TensorFlow 1\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "!git clone --quiet https://github.com/tensorflow/models.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjYEUL1w2wYm",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Install the TFOD API\n",
        "%%bash\n",
        "pip install --upgrade pip\n",
        "cd models/research\n",
        "# Compile protos.\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "# Install TensorFlow Object Detection API.\n",
        "cp object_detection/packages/tf1/setup.py .\n",
        "python -m pip install --use-feature=2020-resolver ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VDEt_UG6dAS",
        "colab_type": "text"
      },
      "source": [
        "## Export to TFLite compatible graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My3UgMuro4av",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Fetch the checkpoints\n",
        "#@markdown Model checkpoints are listed [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md). \n",
        "mobiledet_checkpoint_name = \"ssd_mobiledet_edgetpu_coco\" #@param [\"ssd_mobiledet_cpu_coco\", \"ssd_mobiledet_edgetpu_coco\", \"ssd_mobiledet_dsp_coco\"]\n",
        "\n",
        "checkpoint_dict = {\n",
        "    \"ssd_mobiledet_cpu_coco\": \"http://download.tensorflow.org/models/object_detection/ssdlite_mobiledet_cpu_320x320_coco_2020_05_19.tar.gz\",\n",
        "    \"ssd_mobiledet_edgetpu_coco\": \"http://download.tensorflow.org/models/object_detection/ssdlite_mobiledet_edgetpu_320x320_coco_2020_05_19.tar.gz\",\n",
        "    \"ssd_mobiledet_dsp_coco\": \"http://download.tensorflow.org/models/object_detection/ssdlite_mobiledet_dsp_320x320_coco_2020_05_19.tar.gz\"\n",
        "}\n",
        "\n",
        "folder_name_dict = {\n",
        "    \"ssd_mobiledet_cpu_coco\": \"ssdlite_mobiledet_cpu_320x320_coco_2020_05_19\",\n",
        "    \"ssd_mobiledet_edgetpu_coco\": \"ssdlite_mobiledet_edgetpu_320x320_coco_2020_05_19\",\n",
        "    \"ssd_mobiledet_dsp_coco\": \"ssdlite_mobiledet_dsp_320x320_coco_2020_05_19\"\n",
        "}\n",
        "\n",
        "checkpoint_selected = checkpoint_dict[mobiledet_checkpoint_name]\n",
        "folder_name = folder_name_dict[mobiledet_checkpoint_name]\n",
        "\n",
        "# Get the pre-trained MobileDet checkpoints\n",
        "!rm -rf folder_name\n",
        "!wget -q $checkpoint_selected -O checkpoints.tar.gz\n",
        "!tar -xvf checkpoints.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6MxpD8COMtq",
        "colab_type": "text"
      },
      "source": [
        "**Note** that the checkpoint bundles already come with a TFLite compatible graphs. But the purpose of this notebook is to provide you with a way to adapt the tools for your own models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNHGgnaLpXhc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Export a TFLite compatible model graph\n",
        "#@markdown - For **DSP** & **EdgeTPU** provide `fp32` or `uint8` to the `trained_checkpoint_prefix` and `pipeline_config_path` arguments.\n",
        "precision = \"fp32\" #@param [\"fp32\", \"uint8\"]\n",
        "\n",
        "if \"dsp\" in folder_name or \"edgetpu\" in folder_name:\n",
        "    !python /content/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
        "        --pipeline_config_path=$folder_name/$precision/pipeline.config \\\n",
        "        --trained_checkpoint_prefix=$folder_name/$precision/model.ckpt-400000 \\\n",
        "        --output_directory=$folder_name/$precision \\\n",
        "        --add_postprocessing_op=true\n",
        "    print(\"======Graph generated========\")\n",
        "    !ls -lh $folder_name/$precision/*.pb\n",
        "else:\n",
        "    !python /content/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
        "        --pipeline_config_path=$folder_name/pipeline.config \\\n",
        "        --trained_checkpoint_prefix=$folder_name/model.ckpt-400000 \\\n",
        "        --output_directory=$folder_name \\\n",
        "        --add_postprocessing_op=true\n",
        "    print(\"======Graph generated========\")\n",
        "    !ls -lh $folder_name/*.pb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6RIDGRpN213",
        "colab_type": "text"
      },
      "source": [
        "In each of the respective model folders the graph file would be generated with this name - `tflite_graph.pb`. You can see the size of the graph in the output of the above code block (it should appear below `======Graph generated========`). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR45aPlf5vI-",
        "colab_type": "text"
      },
      "source": [
        "We now need to switch to TensorFlow 2 runtime. We need to restart our Colab runtime first. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tZ6kCnv6ktt",
        "colab_type": "text"
      },
      "source": [
        "## Export to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmDgMRl-2Ggi",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Initial setup\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH5jzM0Dr2JU",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Download the COCO `train2014` dataset and subsample 100 images for it\n",
        "#@markdown **Note**: To run this, please first uncomment the code.\n",
        "#@markdown Additionally, you can skip this step and download a tar'd version of the subsampled images from [here](https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/releases/download/v0.9.0/train_samples_coco.tar.gz) by running `wget -q https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/releases/download/v0.9.0/train_samples_coco.tar.gz`.\n",
        "# from imutils import paths\n",
        "# import random\n",
        "# import shutil\n",
        "# import tqdm\n",
        "# import os\n",
        "\n",
        "# !wget -q http://images.cocodataset.org/zips/train2014.zip\n",
        "# !unzip -qq train2014.zip\n",
        "\n",
        "# train_image_path = list(paths.list_images(\"/content/train2014\"))\n",
        "# random.shuffle(train_image_path)\n",
        "\n",
        "# hundred_images = train_image_path[:100]\n",
        "\n",
        "# !mkdir train_samples\n",
        "\n",
        "# for image_path in tqdm.tqdm(hundred_images):\n",
        "#     image_filename = image_path.split(\"/\")[-1]\n",
        "#     dst_filename = os.path.join(\"train_samples\", image_filename)\n",
        "#     shutil.copy2(image_path, dst_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie90hMh-ycr8",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@markdown If you downloaded the tar file then uncomment and run this code block.\n",
        "# !tar -xvf train_samples_coco.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xchOo2VsOWea",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Select model to quantize\n",
        "model_checkpoint_name = \"ssd_mobiledet_edgetpu_coco_fp32\" #@param [\"ssd_mobiledet_cpu_coco\", \"ssd_mobiledet_edgetpu_coco_fp32\", \"ssd_mobiledet_edgetpu_coco_uint8\", \"ssd_mobiledet_dsp_coco_fp32\", \"ssd_mobiledet_dsp_coco_uint8\"]\n",
        "model_dict = {\n",
        "    \"ssd_mobiledet_cpu_coco\": \"ssdlite_mobiledet_cpu_320x320_coco_2020_05_19/tflite_graph.pb\",\n",
        "    \"ssd_mobiledet_edgetpu_coco_fp32\": \"ssdlite_mobiledet_edgetpu_320x320_coco_2020_05_19/fp32/tflite_graph.pb\",\n",
        "    \"ssd_mobiledet_edgetpu_coco_uint8\": \"ssdlite_mobiledet_edgetpu_320x320_coco_2020_05_19/uint8/tflite_graph.pb\",\n",
        "    \"ssd_mobiledet_dsp_coco_fp32\": \"/content/ssdlite_mobiledet_dsp_320x320_coco_2020_05_19/fp32/tflite_graph.pb\",\n",
        "    \"ssd_mobiledet_dsp_coco_uint8\": \"/content/ssdlite_mobiledet_dsp_320x320_coco_2020_05_19/uint8/tflite_graph.pb\"\n",
        "}\n",
        "model_to_be_quantized = model_dict[model_checkpoint_name]\n",
        "print(f\"We are quantizing: {model_dict[model_checkpoint_name]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmz3NUyUw-2q",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Integer quantization requires a representative dataset\n",
        "# Load up the image paths as a TensorFlow dataset\n",
        "rep_ds = tf.data.Dataset.list_files(\"train_samples/*.jpg\")\n",
        "HEIGHT, WIDTH = 320, 320\n",
        "\n",
        "def representative_dataset_gen():\n",
        "    for image_path in rep_ds:\n",
        "        img = tf.io.read_file(image_path)\n",
        "        img = tf.io.decode_image(img, channels=3)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        resized_img = tf.image.resize(img, (HEIGHT, WIDTH))\n",
        "        resized_img = resized_img[tf.newaxis, :]\n",
        "        yield [resized_img]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28rej2Ot7NKu",
        "colab_type": "text"
      },
      "source": [
        "As the `.pb` file we generated in the earlier step is a frozen graph, we need to use `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph` to convert it to TFLite. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9PR2C9u6qo9",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Quantize and serialize (`uint8` are WIP)\n",
        "#@markdown For `uint8` variants don't use `fp16` and `dr`.\n",
        "#@markdown If you selected `int8` then please be patient. It takes time. \n",
        "quantization_strategy = \"int8\" #@param [\"dr\", \"fp16\", \"int8\"]\n",
        "\n",
        "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\n",
        "    graph_def_file=model_to_be_quantized, \n",
        "    input_arrays=['normalized_input_image_tensor'],\n",
        "    output_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'],\n",
        "    input_shapes={'normalized_input_image_tensor': [1, 320, 320, 3]}\n",
        ")\n",
        "converter.allow_custom_ops = True\n",
        "\n",
        "if quantization_strategy==\"int8\":\n",
        "    converter.representative_dataset = representative_dataset_gen\n",
        "    # Full integer enforcement is not yet supported. \n",
        "    # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    # converter.inference_input_type = tf.uint8\n",
        "    # converter.inference_output_type = tf.uint8\n",
        "\n",
        "if quantization_strategy==\"fp16\":\n",
        "    converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_filename = model_checkpoint_name + \"_\" + quantization_strategy + \".tflite\"\n",
        "open(tflite_filename, 'wb').write(tflite_model)\n",
        "print(f\"TFLite model generated with {quantization_strategy}\")\n",
        "!ls -lh $tflite_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DslhoNhoZcGA",
        "colab_type": "text"
      },
      "source": [
        "## Run inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDwrqpWeb9R7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Imports\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Epl80kRfDd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Utility functions\n",
        "#@markdown Utility functions are sourced from [here](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/raspberry_pi/detect_picamera.py).\n",
        "\n",
        "# Download the COCO labels\n",
        "!wget -q https://dl.google.com/coral/canned_models/coco_labels.txt\n",
        "\n",
        "def load_labels(path):\n",
        "  \"\"\"Loads the labels file. Supports files with or without index numbers.\"\"\"\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "    labels = {}\n",
        "    for row_number, content in enumerate(lines):\n",
        "      pair = re.split(r'[:\\s]+', content.strip(), maxsplit=1)\n",
        "      if len(pair) == 2 and pair[0].strip().isdigit():\n",
        "        labels[int(pair[0])] = pair[1].strip()\n",
        "      else:\n",
        "        labels[row_number] = pair[0].strip()\n",
        "  return labels\n",
        "\n",
        "def set_input_tensor(interpreter, image):\n",
        "  \"\"\"Sets the input tensor.\"\"\"\n",
        "  tensor_index = interpreter.get_input_details()[0]['index']\n",
        "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
        "  input_tensor[:, :] = image\n",
        "\n",
        "\n",
        "def get_output_tensor(interpreter, index):\n",
        "  \"\"\"Returns the output tensor at the given index.\"\"\"\n",
        "  output_details = interpreter.get_output_details()[index]\n",
        "  tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
        "  return tensor\n",
        "\n",
        "\n",
        "def detect_objects(interpreter, image, threshold):\n",
        "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
        "  set_input_tensor(interpreter, image)\n",
        "  interpreter.invoke()\n",
        "\n",
        "  # Get all output details\n",
        "  boxes = get_output_tensor(interpreter, 0)\n",
        "  classes = get_output_tensor(interpreter, 1)\n",
        "  scores = get_output_tensor(interpreter, 2)\n",
        "  count = int(get_output_tensor(interpreter, 3))\n",
        "\n",
        "  results = []\n",
        "  for i in range(count):\n",
        "    if scores[i] >= threshold:\n",
        "      result = {\n",
        "          'bounding_box': boxes[i],\n",
        "          'class_id': classes[i],\n",
        "          'score': scores[i]\n",
        "      }\n",
        "      results.append(result)\n",
        "  return results\n",
        "\n",
        "# Load the labels and define a color bank\n",
        "LABELS = load_labels(\"coco_labels.txt\")\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), \n",
        "                            dtype=\"uint8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zylTA-Q6buZR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Select and load a TFLite model\n",
        "#@markdown TODO: Include direct model download links\n",
        "tflite_model = \"ssd_mobiledet_edgetpu_coco_fp32_int8.tflite\" #@param [\"ssd_mobiledet_cpu_coco_dr.tflite\", \"ssd_mobiledet_cpu_coco_fp16.tflite\", \"ssd_mobiledet_cpu_coco_int8.tflite\", \"ssd_mobiledet_dsp_coco_fp32_int8.tflite\", \"ssd_mobiledet_dsp_coco_uint8_int8.tflite\", \"ssd_mobiledet_edgetpu_coco_fp32_int8.tflite\", \"ssd_mobiledet_edgetpu_coco_uint8_int8.tflite\"]\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "_, HEIGHT, WIDTH, _ = interpreter.get_input_details()[0]['shape']\n",
        "print(f\"Height and width accepted by the model: {HEIGHT, WIDTH}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aETUBJ08bq_z",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Image preprocessing utils\n",
        "def preprocess_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_image(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    original_image = img\n",
        "    resized_img = tf.image.resize(img, (HEIGHT, WIDTH))\n",
        "    resized_img = resized_img[tf.newaxis, :]\n",
        "    return resized_img, original_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X6tLCyOcygf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Supply a path to download an image\n",
        "IMAGE_PATH = \"https://github.com/tensorflow/models/raw/master/research/object_detection/test_images/image2.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "!wget -q -O image.png $IMAGE_PATH\n",
        "Image.open('image.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0Ud2PNpdcLo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Inference utils\n",
        "def display_results(image_path, threshold=0.3):\n",
        "    # Load the input image and preprocess it\n",
        "    preprocessed_image, original_image = preprocess_image(image_path)\n",
        "    # print(preprocessed_image.shape, original_image.shape)\n",
        "\n",
        "    # =============Perform inference=====================\n",
        "    start_time = time.monotonic()\n",
        "    results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
        "    print(f\"Elapsed time: {(time.monotonic() - start_time)*1000} miliseconds\")\n",
        "\n",
        "    # =============Display the results====================\n",
        "    original_numpy = original_image.numpy()\n",
        "    for obj in results:\n",
        "        # Convert the bounding box figures from relative coordinates\n",
        "        # to absolute coordinates based on the original resolution\n",
        "        ymin, xmin, ymax, xmax = obj['bounding_box']\n",
        "        xmin = int(xmin * original_numpy.shape[1])\n",
        "        xmax = int(xmax * original_numpy.shape[1])\n",
        "        ymin = int(ymin * original_numpy.shape[0])\n",
        "        ymax = int(ymax * original_numpy.shape[0])\n",
        "\n",
        "        # Grab the class index for the current iteration\n",
        "        idx = int(obj['class_id'])\n",
        "        # Skip the background\n",
        "        if idx >= len(LABELS):\n",
        "            continue\n",
        "\n",
        "        # draw the bounding box and label on the image\n",
        "        color = [int(c) for c in COLORS[idx]]\n",
        "        cv2.rectangle(original_numpy, (xmin, ymin), (xmax, ymax), \n",
        "                    color, 2)\n",
        "        y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
        "        label = \"{}: {:.2f}%\".format(LABELS[obj['class_id']],\n",
        "            obj['score'] * 100)\n",
        "        cv2.putText(original_numpy, label, (xmin, y),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    # return the final ima\n",
        "    original_int = (original_numpy * 255).astype(np.uint8)\n",
        "    return original_int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWVwjrKvdxIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess image and perform inference\n",
        "resultant_image = display_results(\"image.png\", threshold=0.3)\n",
        "Image.fromarray(resultant_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRlQXQzWfZgU",
        "colab_type": "text"
      },
      "source": [
        "[This example](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/raspberry_pi) provides a utility for running a similar TFLite model on real-time video feeds."
      ]
    }
  ]
}